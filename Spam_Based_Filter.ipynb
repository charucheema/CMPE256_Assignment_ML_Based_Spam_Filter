{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Spam Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charucheema/CMPE256_Assignment_ML_Based_Spam_Filter/blob/main/Spam_Based_Filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GxaiBZtSV74"
      },
      "source": [
        "#import libraries\n",
        "import string\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import nltk \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY4xxDauSipb",
        "outputId": "8869b43f-2d03-469a-cb8b-9c657c67e6b3"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5tvzC97SmuC"
      },
      "source": [
        "Tokenizer for english words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bki7UAGSkzg"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsdDAWxQSreh"
      },
      "source": [
        "# Documents Definition\n",
        "doc1 = \"Free -Coupons for next movie. The above links will take you straight to our partner's site. For more information or to see other offers available, you can also visit the Groupon on the Working Advantage website.\"\n",
        "doc2 = \"Free -Coupons for next movie. The above links will take you straight to our partner's site. For more information or to see other offers available, you can also visit the Groupon on the Working Advantage website.\"\n",
        "doc3 = \"Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP\"\n",
        "doc4 = \"Enter to win $25,000 and get a Free Hotel Night! Just click here for a $1 trial membership in NetMarket, the Internet'spremier discount shopping site: Fast Company EZVenture gives you FREE business articles,PLUS, you could win YOUR CHOICE of a BMW Z3 convertible, $100,000, shares of Microsoft stock, or a home office computer. Go there and get your chances to win now. A crazy-funny-cool trivia book with a $10,000 prize? PLUS chocolate, nail polish, cats, barnyard animals, and more?\"\n",
        "doc5 = \"Dear recipient, Avangar Technologies announces the beginning of a new unprecendented global employment campaign. Due to company's exploding growth Avangar is expanding business to the European region. During last employment campaign over 1500 people worldwide took part in Avangar's business and more than half of them are currently employed by the company. And now we are offering you one more opportunity to earn extra money working with Avangar Technologies. We are looking for honest, responsible, hard-working people that can dedicate 2-4 hours of their time per day and earn extra Â£300-500 weekly. All offered positions are currently part-time and give you a chance to work mainly from home.\"\n",
        "doc6 = \"I know that's an incredible statement, but bear with me while I explain. You have already deleted mail from dozens of \\\"Get Rich Quick\\\" schemes, chain letter offers, and LOTS of other absurd scams that promise to make you rich overnight with no investment and no work. My offer isn't one of those. What I'm offering is a straightforward computer-based service that you can run full-or part-time like a regular business. This service runs auto-matically while you sleep, vacation, or work a \\\"regular\\\" job. It provides a valuable new service for businesses in your area. I'm offering a high-tech, low-maintenance, work-fromanywhere business that can bring in a nice comfortable additional income for your family. I did it for eight years. Since I started inviting others to join me, I've helped over 4000 do the same.\"\n",
        "querySpam = \"Free Click here visit open attachment call this number money Out extra offer available Pension Opportunity Chance Investment Pension\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyKzkmvYSVwz"
      },
      "source": [
        "documents = [doc1, doc2, doc3, doc4, doc5, doc6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWXcBo_GSON7",
        "outputId": "c9be9aab-26a9-4ce7-ce7e-d88f1e6b15e1"
      },
      "source": [
        "#Create a corpus with all input documents to be evaluated and spam query doc\n",
        "'''\n",
        "Please note that spam is the last document in corpus, \n",
        "this information will be instrumental when we compute the similarity between individual documents with spam query document\n",
        "'''\n",
        "corpus = [doc1, doc2, doc3, doc4, doc5, doc6, querySpam]\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")                                                                                                                                                                                                   \n",
        "tfidf = vectorizer.fit_transform(corpus)   \n",
        "similarity_matrix = tfidf * tfidf.T    \n",
        "similarity_array = similarity_matrix.toarray()    \n",
        "similarity_array = similarity_matrix.toarray()    \n",
        "\n",
        "# This similarity matrix shows similarity between all the documents in the corpus\n",
        "print(similarity_array)\n",
        "\n",
        "'''\n",
        " Lets grab the last row from the matrix to get similarity measure between\n",
        " spam query document and all other documents in the corpus.\n",
        "'''\n",
        "\n",
        "print(\"Lets grab the last row from the matrix to get similarity measure between spam query document and all other documents in the corpus.\")\n",
        "print(similarity_array[:,-1])\n",
        "\n",
        "# We will skip the last documentt as that is the spam query document itself\n",
        "for idx, val in enumerate(similarity_array[:,-1][:-1]):\n",
        "    if (val > 0):\n",
        "      print('Document \\\" %s \\\" contains words from spam query documentt' % (corpus[idx]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         1.         0.02323922 0.04465679 0.03621186 0.01830851\n",
            "  0.12547951]\n",
            " [1.         1.         0.02323922 0.04465679 0.03621186 0.01830851\n",
            "  0.12547951]\n",
            " [0.02323922 0.02323922 1.         0.03759994 0.01781925 0.\n",
            "  0.21182068]\n",
            " [0.04465679 0.04465679 0.03759994 1.         0.04096667 0.02285399\n",
            "  0.04554243]\n",
            " [0.03621186 0.03621186 0.01781925 0.04096667 1.         0.08868927\n",
            "  0.10791666]\n",
            " [0.01830851 0.01830851 0.         0.02285399 0.08868927 1.\n",
            "  0.04364964]\n",
            " [0.12547951 0.12547951 0.21182068 0.04554243 0.10791666 0.04364964\n",
            "  1.        ]]\n",
            "Lets grab the last row from the matrix to get similarity measure between spam query document and all other documents in the corpus.\n",
            "[0.12547951 0.12547951 0.21182068 0.04554243 0.10791666 0.04364964\n",
            " 1.        ]\n",
            "Document \" Free -Coupons for next movie. The above links will take you straight to our partner's site. For more information or to see other offers available, you can also visit the Groupon on the Working Advantage website. \" contains words from spam query documentt\n",
            "Document \" Free -Coupons for next movie. The above links will take you straight to our partner's site. For more information or to see other offers available, you can also visit the Groupon on the Working Advantage website. \" contains words from spam query documentt\n",
            "Document \" Our records indicate your Pension is under performing to see higher growth and up to 25% cash release reply PENSION for a free review. To opt out reply STOP \" contains words from spam query documentt\n",
            "Document \" Enter to win $25,000 and get a Free Hotel Night! Just click here for a $1 trial membership in NetMarket, the Internet'spremier discount shopping site: Fast Company EZVenture gives you FREE business articles,PLUS, you could win YOUR CHOICE of a BMW Z3 convertible, $100,000, shares of Microsoft stock, or a home office computer. Go there and get your chances to win now. A crazy-funny-cool trivia book with a $10,000 prize? PLUS chocolate, nail polish, cats, barnyard animals, and more? \" contains words from spam query documentt\n",
            "Document \" Dear recipient, Avangar Technologies announces the beginning of a new unprecendented global employment campaign. Due to company's exploding growth Avangar is expanding business to the European region. During last employment campaign over 1500 people worldwide took part in Avangar's business and more than half of them are currently employed by the company. And now we are offering you one more opportunity to earn extra money working with Avangar Technologies. We are looking for honest, responsible, hard-working people that can dedicate 2-4 hours of their time per day and earn extra Â£300-500 weekly. All offered positions are currently part-time and give you a chance to work mainly from home. \" contains words from spam query documentt\n",
            "Document \" I know that's an incredible statement, but bear with me while I explain. You have already deleted mail from dozens of \"Get Rich Quick\" schemes, chain letter offers, and LOTS of other absurd scams that promise to make you rich overnight with no investment and no work. My offer isn't one of those. What I'm offering is a straightforward computer-based service that you can run full-or part-time like a regular business. This service runs auto-matically while you sleep, vacation, or work a \"regular\" job. It provides a valuable new service for businesses in your area. I'm offering a high-tech, low-maintenance, work-fromanywhere business that can bring in a nice comfortable additional income for your family. I did it for eight years. Since I started inviting others to join me, I've helped over 4000 do the same. \" contains words from spam query documentt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6d5YgZKg3ys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZJehR2npxaQ",
        "outputId": "bc8a87ca-2303-4d9e-bad3-0c36fe8a5fe0"
      },
      "source": [
        "def compute_tf(docs_list):\n",
        "    for doc in docs_list:\n",
        "        doc1_lst = doc.split(\" \")\n",
        "        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n",
        "\n",
        "        for token in doc1_lst:\n",
        "            wordDict_1[token] +=  1\n",
        "        df = pd.DataFrame([wordDict_1])\n",
        "        idx = 0\n",
        "        new_col = [\"Term Frequency\"]    \n",
        "        df.insert(loc=idx, column='Document', value=new_col)\n",
        "\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        print(df)\n",
        "        \n",
        "compute_tf([doc1, doc2, doc3,doc4,doc5,doc6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Document  for  or  site.  Groupon  straight  -Coupons  information  \\\n",
            "0  Term Frequency    1   1      1        1         1         1            1   \n",
            "\n",
            "   above  next  other  can  movie.  you  Working  links  take  website.  our  \\\n",
            "0      1     1      1    1       1    2        1      1     1         1    1   \n",
            "\n",
            "   also  the  For  Free  see  to  will  more  offers  on  visit  partner's  \\\n",
            "0     1    2    1     1    1   2     1     1       1   1      1          1   \n",
            "\n",
            "   available,  The  Advantage  \n",
            "0           1    1          1  \n",
            "         Document  for  or  site.  Groupon  straight  -Coupons  information  \\\n",
            "0  Term Frequency    1   1      1        1         1         1            1   \n",
            "\n",
            "   above  next  other  can  movie.  you  Working  links  take  website.  our  \\\n",
            "0      1     1      1    1       1    2        1      1     1         1    1   \n",
            "\n",
            "   also  the  For  Free  see  to  will  more  offers  on  visit  partner's  \\\n",
            "0     1    2    1     1    1   2     1     1       1   1      1          1   \n",
            "\n",
            "   available,  The  Advantage  \n",
            "0           1    1          1  \n",
            "         Document  your  records  performing  for  reply  indicate  Pension  \\\n",
            "0  Term Frequency     1        1           1    1      2         1        1   \n",
            "\n",
            "   review.  free  Our  is  opt  STOP  see  growth  PENSION  to  up  cash  \\\n",
            "0        1     1    1   1    1     1    1       1        1   2   1     1   \n",
            "\n",
            "   higher  and  out  To  a  25%  release  under  \n",
            "0       1    1    1   1  1    1        1      1  \n",
            "         Document  home  your  with  for  or  trial  shares  computer.  PLUS  \\\n",
            "0  Term Frequency     1     1     1    1   1      1       1          1     1   \n",
            "\n",
            "   barnyard  Go  YOUR  trivia  $100,000,  shopping  Just  Z3  chances  $1  \\\n",
            "0         1   1     1       1          1         1     1   1        1   1   \n",
            "\n",
            "   more?  discount  membership  NetMarket,  site:  Company  prize?  \\\n",
            "0      1         1           1           1      1        1       1   \n",
            "\n",
            "   Internet'spremier  BMW  a  Night!  $10,000  gives  Enter  you  in  of  \\\n",
            "0                  1    1  5       1        1      1      1    2   1   2   \n",
            "\n",
            "   EZVenture  there  book  nail  polish,  articles,PLUS,  stock,  \\\n",
            "0          1      1     1     1        1               1       1   \n",
            "\n",
            "   crazy-funny-cool  CHOICE  could  the  A  Free  get  click  now.  FREE  \\\n",
            "0                 1       1      1    1  1     1    2      1     1     1   \n",
            "\n",
            "   business  to  cats,  Microsoft  convertible,  $25,000  office  chocolate,  \\\n",
            "0         1   2      1          1             1        1       1           1   \n",
            "\n",
            "   and  win  Fast  animals,  here  Hotel  \n",
            "0    3    3     1         1     1      1  \n",
            "         Document  last  earn  opportunity  now  Avangar's  dedicate  \\\n",
            "0  Term Frequency     1     2            1    1          1         1   \n",
            "\n",
            "   hard-working  mainly  announces  half  we  part-time  than  is  And  work  \\\n",
            "0             1       1          1     1   1          1     1   1    1     1   \n",
            "\n",
            "   chance  expanding  positions  one  responsible,  Â£300-500  We  offered  \\\n",
            "0       1          1          1    1             1          1   1        1   \n",
            "\n",
            "   looking  European  working  Due  their  hours  worldwide  Dear  are  \\\n",
            "0        1         1        1    1      1      1          1     1    4   \n",
            "\n",
            "   beginning  took  company.  day  you  exploding  All  new  weekly.  that  \\\n",
            "0          1     1         1    1    2          1    1    1        1     1   \n",
            "\n",
            "   region.  employed  Technologies  with  company's  currently  give  \\\n",
            "0        1         1             1     1          1          2     1   \n",
            "\n",
            "   offering  campaign.  employment  2-4  During  Avangar  the  growth  over  \\\n",
            "0         1          1           2    1       1        3    3       1     1   \n",
            "\n",
            "   extra  and  unprecendented  honest,  for  recipient,  global  1500  them  \\\n",
            "0      2    3               1        1    1           1       1     1     1   \n",
            "\n",
            "   by  time  campaign  can  of  in  from  home.  Technologies.  per  money  \\\n",
            "0   1     1         1    1   3   1     1      1              1    1      1   \n",
            "\n",
            "   business  to  people  more  part  a  \n",
            "0         2   4       2     2     1  2  \n",
            "         Document  dozens  those.  or  auto-matically  comfortable  income  \\\n",
            "0  Term Frequency       1       1   1               1            1       1   \n",
            "\n",
            "   area.  know  work-fromanywhere  computer-based  like  You  scams  service  \\\n",
            "0      1     1                  1               1     1    1      1        3   \n",
            "\n",
            "   promise  part-time  vacation,  is  work  years.  offers,  \"regular\"  \\\n",
            "0        1          1          1   1     1       1        1          1   \n",
            "\n",
            "   business.  one  did  while  have  run  isn't  schemes,  that's  it  This  \\\n",
            "0          1    1    1      2     1    1      1         1       1   1     1   \n",
            "\n",
            "   deleted  join  nice  \"Get  sleep,  others  other  explain.  you  an  but  \\\n",
            "0        1     1     1     1       1       1      1         1    3   1    1   \n",
            "\n",
            "   statement,  work.  Rich  valuable  provides  eight  new  that  \\\n",
            "0           1      1     1         1         1      1    1     3   \n",
            "\n",
            "   straightforward  low-maintenance,  I've  absurd  I'm  me  with  It  \\\n",
            "0                1                 1     1       1    2   1     2   1   \n",
            "\n",
            "   already  no  offering  additional  chain  job.  full-or  mail  offer  rich  \\\n",
            "0        1   2         2           1      1     1        1     1      1     1   \n",
            "\n",
            "   same.  high-tech,  letter  the  Since  businesses  over  and  runs  your  \\\n",
            "0      1           1       1    1      1           1     1    2     1     2   \n",
            "\n",
            "   inviting  I  for  family.  bear  incredible  started  regular  bring  LOTS  \\\n",
            "0         1  4    3        1     1           1        1        1      1     1   \n",
            "\n",
            "   4000  can  of  from  in  me,  make  What  investment  My  overnight  \\\n",
            "0     1    2   3     1   2    1     1     1           1   1          1   \n",
            "\n",
            "   business  to  Quick\"  helped  a  do  \n",
            "0         1   2       1       1  6   1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsdxG4WLp6pG",
        "outputId": "46cdca56-f546-4923-98b5-6e69f805d454"
      },
      "source": [
        "#Normalized Term Frequency\n",
        "def termFrequency(term, document):\n",
        "    normalizeDocument = document.lower().split()\n",
        "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
        "\n",
        "def compute_normalizedtf(documents):\n",
        "    tf_doc = []\n",
        "    for txt in documents:\n",
        "        sentence = txt.split()\n",
        "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
        "        for word in sentence:\n",
        "            norm_tf[word] = termFrequency(word, txt)\n",
        "        tf_doc.append(norm_tf)\n",
        "        df = pd.DataFrame([norm_tf])\n",
        "        idx = 0\n",
        "        new_col = [\"Normalized TF\"]    \n",
        "        df.insert(loc=idx, column='Document', value=new_col)\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        print(df)\n",
        "    return tf_doc\n",
        "\n",
        "tf_doc = compute_normalizedtf([doc1, doc2, doc3,doc4,doc5,doc6])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Document       for        or     site.   Groupon  straight  -Coupons  \\\n",
            "0  Normalized TF  0.055556  0.027778  0.027778  0.027778  0.027778  0.027778   \n",
            "\n",
            "   information     above      next     other       can    movie.       you  \\\n",
            "0     0.027778  0.027778  0.027778  0.027778  0.027778  0.027778  0.055556   \n",
            "\n",
            "    Working     links      take  website.       our      also       the  \\\n",
            "0  0.027778  0.027778  0.027778  0.027778  0.027778  0.027778  0.083333   \n",
            "\n",
            "        For      Free       see        to      will      more    offers  \\\n",
            "0  0.055556  0.027778  0.027778  0.055556  0.027778  0.027778  0.027778   \n",
            "\n",
            "         on     visit  partner's  available,       The  Advantage  \n",
            "0  0.027778  0.027778   0.027778    0.027778  0.083333   0.027778  \n",
            "        Document       for        or     site.   Groupon  straight  -Coupons  \\\n",
            "0  Normalized TF  0.055556  0.027778  0.027778  0.027778  0.027778  0.027778   \n",
            "\n",
            "   information     above      next     other       can    movie.       you  \\\n",
            "0     0.027778  0.027778  0.027778  0.027778  0.027778  0.027778  0.055556   \n",
            "\n",
            "    Working     links      take  website.       our      also       the  \\\n",
            "0  0.027778  0.027778  0.027778  0.027778  0.027778  0.027778  0.083333   \n",
            "\n",
            "        For      Free       see        to      will      more    offers  \\\n",
            "0  0.055556  0.027778  0.027778  0.055556  0.027778  0.027778  0.027778   \n",
            "\n",
            "         on     visit  partner's  available,       The  Advantage  \n",
            "0  0.027778  0.027778   0.027778    0.027778  0.083333   0.027778  \n",
            "        Document      your   records  performing       for     reply  \\\n",
            "0  Normalized TF  0.034483  0.034483    0.034483  0.034483  0.068966   \n",
            "\n",
            "   indicate   Pension   review.      free       Our        is       opt  \\\n",
            "0  0.034483  0.068966  0.034483  0.034483  0.034483  0.034483  0.034483   \n",
            "\n",
            "       STOP       see    growth   PENSION        to        up      cash  \\\n",
            "0  0.034483  0.034483  0.034483  0.068966  0.103448  0.034483  0.034483   \n",
            "\n",
            "     higher       and       out        To         a       25%   release  \\\n",
            "0  0.034483  0.034483  0.034483  0.103448  0.034483  0.034483  0.034483   \n",
            "\n",
            "      under  \n",
            "0  0.034483  \n",
            "        Document      home      your      with       for        or     trial  \\\n",
            "0  Normalized TF  0.012658  0.025316  0.012658  0.012658  0.012658  0.012658   \n",
            "\n",
            "     shares  computer.      PLUS  barnyard        Go      YOUR    trivia  \\\n",
            "0  0.012658   0.012658  0.012658  0.012658  0.012658  0.025316  0.012658   \n",
            "\n",
            "   $100,000,  shopping      Just        Z3   chances        $1     more?  \\\n",
            "0   0.012658  0.012658  0.012658  0.012658  0.012658  0.012658  0.012658   \n",
            "\n",
            "   discount  membership  NetMarket,     site:   Company    prize?  \\\n",
            "0  0.012658    0.012658    0.012658  0.012658  0.012658  0.012658   \n",
            "\n",
            "   Internet'spremier       BMW         a    Night!   $10,000     gives  \\\n",
            "0           0.012658  0.012658  0.075949  0.012658  0.012658  0.012658   \n",
            "\n",
            "      Enter       you        in        of  EZVenture     there      book  \\\n",
            "0  0.012658  0.025316  0.012658  0.025316   0.012658  0.012658  0.012658   \n",
            "\n",
            "       nail   polish,  articles,PLUS,    stock,  crazy-funny-cool    CHOICE  \\\n",
            "0  0.012658  0.012658        0.012658  0.012658          0.012658  0.012658   \n",
            "\n",
            "      could       the         A      Free       get     click      now.  \\\n",
            "0  0.012658  0.012658  0.075949  0.025316  0.025316  0.012658  0.012658   \n",
            "\n",
            "       FREE  business        to     cats,  Microsoft  convertible,   $25,000  \\\n",
            "0  0.025316  0.012658  0.025316  0.012658   0.012658      0.012658  0.012658   \n",
            "\n",
            "     office  chocolate,       and       win      Fast  animals,      here  \\\n",
            "0  0.012658    0.012658  0.037975  0.037975  0.012658  0.012658  0.012658   \n",
            "\n",
            "      Hotel  \n",
            "0  0.012658  \n",
            "        Document      last      earn  opportunity       now  Avangar's  \\\n",
            "0  Normalized TF  0.009259  0.018519     0.009259  0.009259   0.009259   \n",
            "\n",
            "   dedicate  hard-working    mainly  announces      half        we  part-time  \\\n",
            "0  0.009259      0.009259  0.009259   0.009259  0.009259  0.018519   0.009259   \n",
            "\n",
            "       than        is       And      work    chance  expanding  positions  \\\n",
            "0  0.009259  0.009259  0.037037  0.009259  0.009259   0.009259   0.009259   \n",
            "\n",
            "        one  responsible,  Â£300-500        We   offered   looking  European  \\\n",
            "0  0.009259      0.009259   0.009259  0.018519  0.009259  0.009259  0.009259   \n",
            "\n",
            "    working       Due     their     hours  worldwide      Dear       are  \\\n",
            "0  0.009259  0.009259  0.009259  0.009259   0.009259  0.009259  0.037037   \n",
            "\n",
            "   beginning      took  company.       day       you  exploding       All  \\\n",
            "0   0.009259  0.009259  0.009259  0.009259  0.018519   0.009259  0.009259   \n",
            "\n",
            "        new   weekly.      that   region.  employed  Technologies      with  \\\n",
            "0  0.009259  0.009259  0.009259  0.009259  0.009259      0.009259  0.009259   \n",
            "\n",
            "   company's  currently      give  offering  campaign.  employment       2-4  \\\n",
            "0   0.009259   0.018519  0.009259  0.009259   0.009259    0.018519  0.009259   \n",
            "\n",
            "     During   Avangar       the    growth      over     extra       and  \\\n",
            "0  0.009259  0.027778  0.027778  0.009259  0.009259  0.018519  0.037037   \n",
            "\n",
            "   unprecendented   honest,       for  recipient,    global      1500  \\\n",
            "0        0.009259  0.009259  0.009259    0.009259  0.009259  0.009259   \n",
            "\n",
            "       them        by      time  campaign       can        of        in  \\\n",
            "0  0.009259  0.009259  0.009259  0.009259  0.009259  0.027778  0.009259   \n",
            "\n",
            "       from     home.  Technologies.       per     money  business        to  \\\n",
            "0  0.009259  0.009259       0.009259  0.009259  0.009259  0.018519  0.037037   \n",
            "\n",
            "     people      more      part         a  \n",
            "0  0.018519  0.018519  0.009259  0.018519  \n",
            "        Document    dozens    those.        or  auto-matically  comfortable  \\\n",
            "0  Normalized TF  0.007407  0.007407  0.007407        0.007407     0.007407   \n",
            "\n",
            "     income     area.      know  work-fromanywhere  computer-based      like  \\\n",
            "0  0.007407  0.007407  0.007407           0.007407        0.007407  0.007407   \n",
            "\n",
            "       You     scams   service   promise  part-time  vacation,        is  \\\n",
            "0  0.02963  0.007407  0.022222  0.007407   0.007407   0.007407  0.007407   \n",
            "\n",
            "       work    years.   offers,  \"regular\"  business.       one       did  \\\n",
            "0  0.007407  0.007407  0.007407   0.007407   0.007407  0.007407  0.007407   \n",
            "\n",
            "      while      have       run     isn't  schemes,    that's        it  \\\n",
            "0  0.014815  0.007407  0.007407  0.007407  0.007407  0.007407  0.014815   \n",
            "\n",
            "       This   deleted      join      nice      \"Get    sleep,    others  \\\n",
            "0  0.007407  0.007407  0.007407  0.007407  0.007407  0.007407  0.007407   \n",
            "\n",
            "      other  explain.      you        an       but  statement,     work.  \\\n",
            "0  0.007407  0.007407  0.02963  0.007407  0.007407    0.007407  0.007407   \n",
            "\n",
            "       Rich  valuable  provides     eight       new      that  \\\n",
            "0  0.014815  0.007407  0.007407  0.007407  0.007407  0.022222   \n",
            "\n",
            "   straightforward  low-maintenance,      I've    absurd       I'm        me  \\\n",
            "0         0.007407          0.007407  0.007407  0.007407  0.014815  0.007407   \n",
            "\n",
            "       with        It   already        no  offering  additional     chain  \\\n",
            "0  0.014815  0.014815  0.007407  0.014815  0.014815    0.007407  0.007407   \n",
            "\n",
            "       job.   full-or      mail     offer      rich     same.  high-tech,  \\\n",
            "0  0.007407  0.007407  0.007407  0.007407  0.014815  0.007407    0.007407   \n",
            "\n",
            "     letter       the     Since  businesses      over       and      runs  \\\n",
            "0  0.007407  0.007407  0.007407    0.007407  0.007407  0.014815  0.007407   \n",
            "\n",
            "       your  inviting        I       for   family.      bear  incredible  \\\n",
            "0  0.014815  0.007407  0.02963  0.022222  0.007407  0.007407    0.007407   \n",
            "\n",
            "    started   regular     bring      LOTS      4000       can        of  \\\n",
            "0  0.007407  0.007407  0.007407  0.007407  0.007407  0.014815  0.022222   \n",
            "\n",
            "       from        in       me,      make      What  investment        My  \\\n",
            "0  0.007407  0.014815  0.007407  0.007407  0.007407    0.007407  0.007407   \n",
            "\n",
            "   overnight  business        to    Quick\"    helped         a        do  \n",
            "0   0.007407  0.007407  0.014815  0.007407  0.007407  0.044444  0.007407  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijqQsPvcqYKZ",
        "outputId": "516cac10-0fed-458d-bb4a-5315cf50b7b9"
      },
      "source": [
        "def inverseDocumentFrequency(term, allDocuments):\n",
        "    numDocumentsWithThisTerm = 0\n",
        "    for doc in range (0, len(allDocuments)):\n",
        "        if term.lower() in allDocuments[doc].lower().split():\n",
        "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
        " \n",
        "    if numDocumentsWithThisTerm > 0:\n",
        "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
        "    else:\n",
        "        return 1.0\n",
        "    \n",
        "def compute_idf(documents):\n",
        "    idf_dict = {}\n",
        "    for doc in documents:\n",
        "        sentence = doc.split()\n",
        "        for word in sentence:\n",
        "            idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
        "    return idf_dict\n",
        "idf_dict = compute_idf([doc1, doc2, doc3, doc4,doc5,doc6])\n",
        "\n",
        "compute_idf([doc1, doc2, doc3,doc4,doc5,doc6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\"Get': 2.791759469228055,\n",
              " '\"regular\"': 2.791759469228055,\n",
              " '$1': 2.791759469228055,\n",
              " '$10,000': 2.791759469228055,\n",
              " '$100,000,': 2.791759469228055,\n",
              " '$25,000': 2.791759469228055,\n",
              " '-Coupons': 2.09861228866811,\n",
              " '1500': 2.791759469228055,\n",
              " '2-4': 2.791759469228055,\n",
              " '25%': 2.791759469228055,\n",
              " '4000': 2.791759469228055,\n",
              " 'A': 1.4054651081081644,\n",
              " 'Advantage': 2.09861228866811,\n",
              " 'All': 2.791759469228055,\n",
              " 'And': 1.4054651081081644,\n",
              " 'Avangar': 2.791759469228055,\n",
              " \"Avangar's\": 2.791759469228055,\n",
              " 'BMW': 2.791759469228055,\n",
              " 'CHOICE': 2.791759469228055,\n",
              " 'Company': 2.791759469228055,\n",
              " 'Dear': 2.791759469228055,\n",
              " 'Due': 2.791759469228055,\n",
              " 'During': 2.791759469228055,\n",
              " 'EZVenture': 2.791759469228055,\n",
              " 'Enter': 2.791759469228055,\n",
              " 'European': 2.791759469228055,\n",
              " 'FREE': 1.4054651081081644,\n",
              " 'Fast': 2.791759469228055,\n",
              " 'For': 1.0,\n",
              " 'Free': 1.4054651081081644,\n",
              " 'Go': 2.791759469228055,\n",
              " 'Groupon': 2.09861228866811,\n",
              " 'Hotel': 2.791759469228055,\n",
              " 'I': 2.791759469228055,\n",
              " \"I'm\": 2.791759469228055,\n",
              " \"I've\": 2.791759469228055,\n",
              " \"Internet'spremier\": 2.791759469228055,\n",
              " 'It': 2.791759469228055,\n",
              " 'Just': 2.791759469228055,\n",
              " 'LOTS': 2.791759469228055,\n",
              " 'Microsoft': 2.791759469228055,\n",
              " 'My': 2.791759469228055,\n",
              " 'NetMarket,': 2.791759469228055,\n",
              " 'Night!': 2.791759469228055,\n",
              " 'Our': 1.6931471805599454,\n",
              " 'PENSION': 2.791759469228055,\n",
              " 'PLUS': 2.791759469228055,\n",
              " 'Pension': 2.791759469228055,\n",
              " 'Quick\"': 2.791759469228055,\n",
              " 'Rich': 2.791759469228055,\n",
              " 'STOP': 2.791759469228055,\n",
              " 'Since': 2.791759469228055,\n",
              " 'Technologies': 2.791759469228055,\n",
              " 'Technologies.': 2.791759469228055,\n",
              " 'The': 1.1823215567939547,\n",
              " 'This': 2.791759469228055,\n",
              " 'To': 1.0,\n",
              " 'We': 2.791759469228055,\n",
              " 'What': 2.791759469228055,\n",
              " 'Working': 1.6931471805599454,\n",
              " 'YOUR': 1.6931471805599454,\n",
              " 'You': 1.1823215567939547,\n",
              " 'Z3': 2.791759469228055,\n",
              " 'a': 1.4054651081081644,\n",
              " 'above': 2.09861228866811,\n",
              " 'absurd': 2.791759469228055,\n",
              " 'additional': 2.791759469228055,\n",
              " 'already': 2.791759469228055,\n",
              " 'also': 2.09861228866811,\n",
              " 'an': 2.791759469228055,\n",
              " 'and': 1.4054651081081644,\n",
              " 'animals,': 2.791759469228055,\n",
              " 'announces': 2.791759469228055,\n",
              " 'are': 2.791759469228055,\n",
              " 'area.': 2.791759469228055,\n",
              " 'articles,PLUS,': 2.791759469228055,\n",
              " 'auto-matically': 2.791759469228055,\n",
              " 'available,': 2.09861228866811,\n",
              " 'barnyard': 2.791759469228055,\n",
              " 'bear': 2.791759469228055,\n",
              " 'beginning': 2.791759469228055,\n",
              " 'book': 2.791759469228055,\n",
              " 'bring': 2.791759469228055,\n",
              " 'business': 1.6931471805599454,\n",
              " 'business.': 2.791759469228055,\n",
              " 'businesses': 2.791759469228055,\n",
              " 'but': 2.791759469228055,\n",
              " 'by': 2.791759469228055,\n",
              " 'campaign': 2.791759469228055,\n",
              " 'campaign.': 2.791759469228055,\n",
              " 'can': 1.4054651081081644,\n",
              " 'cash': 2.791759469228055,\n",
              " 'cats,': 2.791759469228055,\n",
              " 'chain': 2.791759469228055,\n",
              " 'chance': 2.791759469228055,\n",
              " 'chances': 2.791759469228055,\n",
              " 'chocolate,': 2.791759469228055,\n",
              " 'click': 2.791759469228055,\n",
              " 'comfortable': 2.791759469228055,\n",
              " \"company's\": 2.791759469228055,\n",
              " 'company.': 2.791759469228055,\n",
              " 'computer-based': 2.791759469228055,\n",
              " 'computer.': 2.791759469228055,\n",
              " 'convertible,': 2.791759469228055,\n",
              " 'could': 2.791759469228055,\n",
              " 'crazy-funny-cool': 2.791759469228055,\n",
              " 'currently': 2.791759469228055,\n",
              " 'day': 2.791759469228055,\n",
              " 'dedicate': 2.791759469228055,\n",
              " 'deleted': 2.791759469228055,\n",
              " 'did': 2.791759469228055,\n",
              " 'discount': 2.791759469228055,\n",
              " 'do': 2.791759469228055,\n",
              " 'dozens': 2.791759469228055,\n",
              " 'earn': 2.791759469228055,\n",
              " 'eight': 2.791759469228055,\n",
              " 'employed': 2.791759469228055,\n",
              " 'employment': 2.791759469228055,\n",
              " 'expanding': 2.791759469228055,\n",
              " 'explain.': 2.791759469228055,\n",
              " 'exploding': 2.791759469228055,\n",
              " 'extra': 2.791759469228055,\n",
              " 'family.': 2.791759469228055,\n",
              " 'for': 1.0,\n",
              " 'free': 1.4054651081081644,\n",
              " 'from': 2.09861228866811,\n",
              " 'full-or': 2.791759469228055,\n",
              " 'get': 2.791759469228055,\n",
              " 'give': 2.791759469228055,\n",
              " 'gives': 2.791759469228055,\n",
              " 'global': 2.791759469228055,\n",
              " 'growth': 2.09861228866811,\n",
              " 'half': 2.791759469228055,\n",
              " 'hard-working': 2.791759469228055,\n",
              " 'have': 2.791759469228055,\n",
              " 'helped': 2.791759469228055,\n",
              " 'here': 2.791759469228055,\n",
              " 'high-tech,': 2.791759469228055,\n",
              " 'higher': 2.791759469228055,\n",
              " 'home': 2.791759469228055,\n",
              " 'home.': 2.791759469228055,\n",
              " 'honest,': 2.791759469228055,\n",
              " 'hours': 2.791759469228055,\n",
              " 'in': 1.6931471805599454,\n",
              " 'income': 2.791759469228055,\n",
              " 'incredible': 2.791759469228055,\n",
              " 'indicate': 2.791759469228055,\n",
              " 'information': 2.09861228866811,\n",
              " 'investment': 2.791759469228055,\n",
              " 'inviting': 2.791759469228055,\n",
              " 'is': 1.6931471805599454,\n",
              " \"isn't\": 2.791759469228055,\n",
              " 'it': 2.791759469228055,\n",
              " 'job.': 2.791759469228055,\n",
              " 'join': 2.791759469228055,\n",
              " 'know': 2.791759469228055,\n",
              " 'last': 2.791759469228055,\n",
              " 'letter': 2.791759469228055,\n",
              " 'like': 2.791759469228055,\n",
              " 'links': 2.09861228866811,\n",
              " 'looking': 2.791759469228055,\n",
              " 'low-maintenance,': 2.791759469228055,\n",
              " 'mail': 2.791759469228055,\n",
              " 'mainly': 2.791759469228055,\n",
              " 'make': 2.791759469228055,\n",
              " 'me': 2.791759469228055,\n",
              " 'me,': 2.791759469228055,\n",
              " 'membership': 2.791759469228055,\n",
              " 'money': 2.791759469228055,\n",
              " 'more': 1.6931471805599454,\n",
              " 'more?': 2.791759469228055,\n",
              " 'movie.': 2.09861228866811,\n",
              " 'nail': 2.791759469228055,\n",
              " 'new': 2.09861228866811,\n",
              " 'next': 2.09861228866811,\n",
              " 'nice': 2.791759469228055,\n",
              " 'no': 2.791759469228055,\n",
              " 'now': 2.791759469228055,\n",
              " 'now.': 2.791759469228055,\n",
              " 'of': 1.6931471805599454,\n",
              " 'offer': 2.791759469228055,\n",
              " 'offered': 2.791759469228055,\n",
              " 'offering': 2.09861228866811,\n",
              " 'offers': 2.09861228866811,\n",
              " 'offers,': 2.791759469228055,\n",
              " 'office': 2.791759469228055,\n",
              " 'on': 2.09861228866811,\n",
              " 'one': 2.09861228866811,\n",
              " 'opportunity': 2.791759469228055,\n",
              " 'opt': 2.791759469228055,\n",
              " 'or': 1.4054651081081644,\n",
              " 'other': 1.6931471805599454,\n",
              " 'others': 2.791759469228055,\n",
              " 'our': 1.6931471805599454,\n",
              " 'out': 2.791759469228055,\n",
              " 'over': 2.09861228866811,\n",
              " 'overnight': 2.791759469228055,\n",
              " 'part': 2.791759469228055,\n",
              " 'part-time': 2.09861228866811,\n",
              " \"partner's\": 2.09861228866811,\n",
              " 'people': 2.791759469228055,\n",
              " 'per': 2.791759469228055,\n",
              " 'performing': 2.791759469228055,\n",
              " 'polish,': 2.791759469228055,\n",
              " 'positions': 2.791759469228055,\n",
              " 'prize?': 2.791759469228055,\n",
              " 'promise': 2.791759469228055,\n",
              " 'provides': 2.791759469228055,\n",
              " 'recipient,': 2.791759469228055,\n",
              " 'records': 2.791759469228055,\n",
              " 'region.': 2.791759469228055,\n",
              " 'regular': 2.791759469228055,\n",
              " 'release': 2.791759469228055,\n",
              " 'reply': 2.791759469228055,\n",
              " 'responsible,': 2.791759469228055,\n",
              " 'review.': 2.791759469228055,\n",
              " 'rich': 2.791759469228055,\n",
              " 'run': 2.791759469228055,\n",
              " 'runs': 2.791759469228055,\n",
              " 'same.': 2.791759469228055,\n",
              " 'scams': 2.791759469228055,\n",
              " 'schemes,': 2.791759469228055,\n",
              " 'see': 1.6931471805599454,\n",
              " 'service': 2.791759469228055,\n",
              " 'shares': 2.791759469228055,\n",
              " 'shopping': 2.791759469228055,\n",
              " 'site.': 2.09861228866811,\n",
              " 'site:': 2.791759469228055,\n",
              " 'sleep,': 2.791759469228055,\n",
              " 'started': 2.791759469228055,\n",
              " 'statement,': 2.791759469228055,\n",
              " 'stock,': 2.791759469228055,\n",
              " 'straight': 2.09861228866811,\n",
              " 'straightforward': 2.791759469228055,\n",
              " 'take': 2.09861228866811,\n",
              " 'than': 2.791759469228055,\n",
              " 'that': 2.09861228866811,\n",
              " \"that's\": 2.791759469228055,\n",
              " 'the': 1.1823215567939547,\n",
              " 'their': 2.791759469228055,\n",
              " 'them': 2.791759469228055,\n",
              " 'there': 2.791759469228055,\n",
              " 'those.': 2.791759469228055,\n",
              " 'time': 2.791759469228055,\n",
              " 'to': 1.0,\n",
              " 'took': 2.791759469228055,\n",
              " 'trial': 2.791759469228055,\n",
              " 'trivia': 2.791759469228055,\n",
              " 'under': 2.791759469228055,\n",
              " 'unprecendented': 2.791759469228055,\n",
              " 'up': 2.791759469228055,\n",
              " 'vacation,': 2.791759469228055,\n",
              " 'valuable': 2.791759469228055,\n",
              " 'visit': 2.09861228866811,\n",
              " 'we': 2.791759469228055,\n",
              " 'website.': 2.09861228866811,\n",
              " 'weekly.': 2.791759469228055,\n",
              " 'while': 2.791759469228055,\n",
              " 'will': 2.09861228866811,\n",
              " 'win': 2.791759469228055,\n",
              " 'with': 1.6931471805599454,\n",
              " 'work': 2.09861228866811,\n",
              " 'work-fromanywhere': 2.791759469228055,\n",
              " 'work.': 2.791759469228055,\n",
              " 'working': 1.6931471805599454,\n",
              " 'worldwide': 2.791759469228055,\n",
              " 'years.': 2.791759469228055,\n",
              " 'you': 1.1823215567939547,\n",
              " 'your': 1.6931471805599454,\n",
              " 'Â£300-500': 2.791759469228055}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP2N07-NqsCU",
        "outputId": "244b0c7e-f17a-4dbc-96ab-b6359cb124ec"
      },
      "source": [
        "# tf-idf score across all docs for the query spam\n",
        "def compute_tfidf_with_alldocs(documents , querySpam):\n",
        "    tf_idf = []\n",
        "    index = 0\n",
        "    query_tokens = querySpam.split()\n",
        "    query_tokens = list(set(query_tokens))\n",
        "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
        "    for doc in documents:\n",
        "        df['doc'] = np.arange(0 , len(documents))\n",
        "        doc_num = tf_doc[index]\n",
        "        sentence = doc.split()\n",
        "        for word in sentence:\n",
        "            for text in query_tokens:\n",
        "                if(text == word):\n",
        "                    idx = sentence.index(word)\n",
        "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
        "                    tf_idf.append(tf_idf_score)\n",
        "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
        "        index += 1\n",
        "    df.fillna(0 , axis=1, inplace=True)\n",
        "    return tf_idf , df\n",
        "            \n",
        "documents = [doc1, doc2, doc3,doc4,doc5,doc6]\n",
        "tf_idf , df = compute_tfidf_with_alldocs(documents , querySpam)\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(df)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   doc    offer  Out  Click  call     extra  attachment  this  number  \\\n",
            "0    0  0.00000    0      0     0  0.000000           0     0       0   \n",
            "1    1  0.00000    0      0     0  0.000000           0     0       0   \n",
            "2    2  0.00000    0      0     0  0.000000           0     0       0   \n",
            "3    3  0.00000    0      0     0  0.000000           0     0       0   \n",
            "4    4  0.00000    0      0     0  0.051699           0     0       0   \n",
            "5    5  0.02068    0      0     0  0.000000           0     0       0   \n",
            "\n",
            "    Pension  Investment  Opportunity     visit  available      here  Chance  \\\n",
            "0  0.000000           0            0  0.058295          0  0.000000       0   \n",
            "1  0.000000           0            0  0.058295          0  0.000000       0   \n",
            "2  0.192535           0            0  0.000000          0  0.000000       0   \n",
            "3  0.000000           0            0  0.000000          0  0.035339       0   \n",
            "4  0.000000           0            0  0.000000          0  0.000000       0   \n",
            "5  0.000000           0            0  0.000000          0  0.000000       0   \n",
            "\n",
            "   open      Free    money  \n",
            "0     0  0.039041  0.00000  \n",
            "1     0  0.039041  0.00000  \n",
            "2     0  0.000000  0.00000  \n",
            "3     0  0.035581  0.00000  \n",
            "4     0  0.000000  0.02585  \n",
            "5     0  0.000000  0.00000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o44S071WSss5"
      },
      "source": [
        "documents = [d1, d2, d3, d4, d5, d6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3gUW6lFSvN6"
      },
      "source": [
        "# Scikit Learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZe-sPU5SzR3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "9280bb4b-bf33-4b00-b4cb-7d2b639fb6ca"
      },
      "source": [
        "# Create the Document Term Matrix\n",
        "print (stopwords.words('english'))\n",
        "tfidfVectorizer = TfidfVectorizer(stop_words='english')\n",
        "sparse_matrix = tfidfVectorizer.fit_transform(documents)\n",
        "print(sparse_matrix.get_feature_names_out())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-5dc5afa32516>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtfidfVectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msparse_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidfVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: get_feature_names_out not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h-Jf4aIA9QJ"
      },
      "source": [
        "DF = {}\n",
        "for i in range(len(documents)):\n",
        "    tokens = documents[i]\n",
        "    for w in tokens:\n",
        "        try:\n",
        "            DF[w].add(i)\n",
        "        except:\n",
        "            DF[w] = {i}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "kT2cH7YZB468",
        "outputId": "bc9a714f-8240-4698-eedc-c82b615abb64"
      },
      "source": [
        "for i in DF:\n",
        "  DF[i]=len(DF[i])\n",
        "DF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-84bdb382602e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SlNOT77IG--",
        "outputId": "3406771a-b2bb-4f38-e547-30020cc6bc46"
      },
      "source": [
        "total_vocab = [x for x in DF]\n",
        "print(total_vocab)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['F', 'r', 'e', ' ', '-', 'C', 'o', 'u', 'p', 'n', 's', 'f', 'x', 't', 'm', 'v', 'i', '.', 'T', 'h', 'a', 'b', 'l', 'k', 'w', 'y', 'g', \"'\", ',', 'c', 'G', 'W', 'A', 'd', 'O', 'P', '2', '5', '%', 'E', 'N', 'S', 'I', '$', '0', 'H', '!', 'J', '1', 'M', ':', 'Z', 'V', 'R', 'L', 'U', 'Y', 'B', '3', 'z', '?', 'D', '4', 'Â', '£', '\"', 'Q', 'j']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "R_RE-OWNIhMk",
        "outputId": "084709cf-f9eb-422a-cb49-e16702d7ba49"
      },
      "source": [
        "tf_idf = {}\n",
        "for i in range(N):\n",
        "    tokens = documents[i]\n",
        "    counter = Counter(tokens + processed_title[i])\n",
        "    for token in np.unique(tokens):\n",
        "        tf = counter[token]/words_count\n",
        "        df = doc_freq(token)\n",
        "        idf = np.log(N/(df+1))\n",
        "        tf_idf[doc, token] = tf*idf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4bc5a03da6ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprocessed_title\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'N' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIhqDFe_TODO"
      },
      "source": [
        "Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhR3jGMLS2dv",
        "outputId": "1c59f9b0-5491-4f09-d350-c92c4ac72a7d"
      },
      "source": [
        "doc_term_matrix = sparse_matrix.todense()\n",
        "print(doc_term_matrix.shape)\n",
        "df = pd.DataFrame(doc_term_matrix,\n",
        "                   columns=tfidfVectorizer.get_feature_names(),\n",
        "                   index=['d1', 'd2', 'd3', 'd4', 'd5', 'd6'])\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 174)\n",
            "         000        10       100  ...  worldwide     years        z3\n",
            "d1  0.000000  0.000000  0.000000  ...   0.000000  0.000000  0.000000\n",
            "d2  0.000000  0.000000  0.000000  ...   0.000000  0.000000  0.000000\n",
            "d3  0.000000  0.000000  0.000000  ...   0.000000  0.000000  0.000000\n",
            "d4  0.369118  0.123039  0.123039  ...   0.000000  0.000000  0.123039\n",
            "d5  0.000000  0.000000  0.000000  ...   0.106858  0.000000  0.000000\n",
            "d6  0.000000  0.000000  0.000000  ...   0.000000  0.109008  0.000000\n",
            "\n",
            "[6 rows x 174 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2iOKmzIS_S4",
        "outputId": "626c7df4-6fb9-41ba-fab8-2206378e54d3"
      },
      "source": [
        "# Compute Cosine Similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity(df, df))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         1.         0.0268553  0.04803192 0.03379612 0.01723806]\n",
            " [1.         1.         0.0268553  0.04803192 0.03379612 0.01723806]\n",
            " [0.0268553  0.0268553  1.         0.03916376 0.01661755 0.        ]\n",
            " [0.04803192 0.04803192 0.03916376 1.         0.03912589 0.02187566]\n",
            " [0.03379612 0.03379612 0.01661755 0.03912589 1.         0.08499357]\n",
            " [0.01723806 0.01723806 0.         0.02187566 0.08499357 1.        ]]\n"
          ]
        }
      ]
    }
  ]
}